from tensorflow import keras
from sklearn.model_selection import train_test_split
import tensorflow.keras.regularizers
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding
from tensorflow.keras.layers import LSTM
from tensorflow.keras.datasets import imdb
import numpy as np
import matplotlib as matplot
import tensorflow as tf
from matplotlib import pyplot as plt
import tensorflow as tf
#from tensorflow.keras.models import load_model

# from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping

# set up hyperparameters
# max_features = 50000
maxlen = 500
batch_size = 128
VOCAB_size = 4000


# input_length=500
# LSTM player=60
# embedding player=32
# dropout=0.1
# lr=0.005
# l2 regulizer=0.001

class PrintWeightsCallback(tf.keras.callbacks.Callback):
    def on_epoch_begin(self, epoch, logs=None):
        l1_w = self.model.layers[1].weights[0].numpy()[:6, 0]
        print(f'layer1: {l1_w}')


print('Loading data...')  # just in case accident happens, can find out the problem more easily(in which part)

(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=4000)
# np.random.shuffle(train_data)
# np.random.shuffle(test_data)
# np.random.shuffle(train_labels)
# np.random.shuffle(train_labels)
a = np.append(train_data, test_data)
# np.random.shuffle(a)
b = np.append(train_labels, test_labels)
# np.random.shuffle(b)
x_train = a[:int(0.7 * len(a))]
y_train = b[:int(0.7 * len(b))]
x_test = a[int(0.7 * len(a)):]
y_test = b[int(0.7 * len(b)):]

x_train = sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = sequence.pad_sequences(x_test, maxlen=maxlen)
print('x_train shape:', x_train.shape)
print('x_test shape:', x_test.shape)

# set up keras LSTM nn
model = Sequential()
model.add(Embedding(VOCAB_size, output_dim=32, input_length=500))
model.add(LSTM(60, dropout=0.1, kernel_regularizer=tensorflow.keras.regularizers.l2(0.001)))
model.add(Dense(1, activation='sigmoid'))
opt = tensorflow.keras.optimizers.Adam(lr=0.005, epsilon=None, decay=0.0, amsgrad=False)

model.compile(loss='binary_crossentropy',
              optimizer=opt,
              metrics=['accuracy'])

# start training (epochs can be changed)
# for e in range(0,1000):
# epochs=e
history = model.fit(x_train, y_train,
                    batch_size=batch_size,
                    epochs=700,
                    validation_data=(x_test, y_test),
                    callbacks=[PrintWeightsCallback()])

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(len(acc))

plt.plot(epochs, acc, 'b', label='Training accuracy')
plt.plot(epochs, val_acc, 'r', label='validation accuracy')
plt.title('Training and validation accuracy')
plt.legend(loc='lower right')
plt.figure()

plt.plot(epochs, loss, 'r', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()

