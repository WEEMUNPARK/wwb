from __future__ import print_function
from sklearn.model_selection import train_test_split
import tensorflow.keras.regularizers
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding
from tensorflow.keras.layers import LSTM
from tensorflow.keras.datasets import imdb
import numpy as np
#import matplotlib as matplot

#set up hyperparameters
#max_features = 50000
maxlen=500
batch_size = 128
VOCAB_size = 4000
#input_length=500
#LSTM player=60
#embedding player=32
#dropout=0.1
#lr=0.005
#l2 regulizer=0.001


print('Loading data...') #just in case accident happens, can find out the problem more easily(in which part)

(train_data,train_labels),(test_data,test_labels) = imdb.load_data(num_words=4000)
a=np.append(train_data,test_data)
b=np.append(train_labels,test_labels)
x_train=a[:int(0.7*len(a))]
y_train=b[:int(0.7*len(b))]
x_test=a[int(0.7*len(a)):]
y_test=b[int(0.7*len(b)):]

x_train = sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = sequence.pad_sequences(x_test, maxlen=maxlen)
print('x_train shape:', x_train.shape)
print('x_test shape:', x_test.shape)

#set up keras LSTM nn
model = Sequential()
model.add(Embedding(VOCAB_size, output_dim=32, input_length= 500))
model.add(LSTM(60, dropout=0.1, kernel_regularizer=keras.regularizers.l2(0.001)))
model.add(Dense(1, activation='sigmoid'))
opt = tensorflow.keras.optimizers.Adam(lr=0.005)

model.compile(loss='binary_crossentropy',
              optimizer=opt,
              metrics=['accuracy'])



#start training (epochs can be changed)
#for e in range(0,1000):
    #epochs=e
model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=1000,
          validation_data=(x_test, y_test))
score, acc = model.evaluate(x_test, y_test,
                            batch_size=batch_size)
print('Test score:', score)
print('Test accuracy:', acc)
